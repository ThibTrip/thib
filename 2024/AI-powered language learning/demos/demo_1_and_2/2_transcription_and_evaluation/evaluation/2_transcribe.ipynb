{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ffa1b0-2968-4cb6-855d-95063b00b050",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlalchemy as sqla\n",
    "import tempfile\n",
    "import time\n",
    "from faster_whisper import WhisperModel\n",
    "from loguru import logger\n",
    "from pangres import upsert\n",
    "from pathlib import Path\n",
    "from plumbum import local\n",
    "from sqlalchemy import create_engine, Connection, text\n",
    "from typing import Protocol\n",
    "# local imports\n",
    "import faster_whisper_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b722ca01-cade-4c70-98ad-1537bc302fab",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2be55d5-3d14-43a8-b4fa-e0eb63951c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common configuration\n",
    "LANGUAGE = 'it'\n",
    "BASE_PATH = Path(LANGUAGE).resolve()\n",
    "TABLE_NAME = 'transcriptions'\n",
    "DB_PATH = 'cv.sqlite3'\n",
    "COMMON_VOICES_VERSION = '17.0'\n",
    "ENGINE = create_engine(f'sqlite:///{DB_PATH}')\n",
    "POETRY_CMD = local['poetry']\n",
    "\n",
    "# tools specific configuration\n",
    "AUTOSUB_LANGUAGE_ASR = 'it-it'  # autosub needs localized language e.g. `it-it` or `uk-ua`\n",
    "WHISPER_LANGUAGE_ASR = LANGUAGE\n",
    "WHISPER_MODEL_NAME = 'large-v3'\n",
    "# if \"cuda\" is installed, use the first line instead of the second, it will be **a lot** faster\n",
    "# WHISPER_MODEL = WhisperModel(model_size_or_path=WHISPER_MODEL_NAME, device=\"cuda\", compute_type=\"float16\")\n",
    "WHISPER_MODEL = WhisperModel(model_size_or_path=WHISPER_MODEL_NAME,device='cpu', compute_type=\"float32\")\n",
    "WHISPER_CONDITION_ON_PREVIOUS_TEXT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd49a6-133d-4c9f-a644-7972c1f5ce57",
   "metadata": {},
   "source": [
    "# Transcribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e9150e-73bb-42e5-8950-31939f0f2b1c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class Transcriber(Protocol):\n",
    "    asr_tool: str\n",
    "\n",
    "    @staticmethod\n",
    "    def transcribe(audio_path: Path | str) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class AutosubTranscriber:\n",
    "    asr_tool = 'autosub'\n",
    "\n",
    "    @staticmethod\n",
    "    def transcribe(audio_path: Path | str) -> str:\n",
    "    \n",
    "        temp_path = tempfile.mktemp(suffix='.txt', dir='.')\n",
    "        # autosub renames the output path provided via parameter `-o`\n",
    "        # slightly to include the language... -_-\"\n",
    "        output_path_autosub = temp_path[:-4] + f'.{AUTOSUB_LANGUAGE_ASR}.txt'\n",
    "\n",
    "        try:\n",
    "            POETRY_CMD['run', 'autosub', '-S', AUTOSUB_LANGUAGE_ASR, '-i', str(audio_path), '-o', temp_path]()\n",
    "            with open(output_path_autosub, 'r') as fh:\n",
    "                transcription = fh.read().strip()\n",
    "        finally:\n",
    "            if os.path.exists(output_path_autosub):\n",
    "                os.remove(output_path_autosub)\n",
    "    \n",
    "        return transcription\n",
    "\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    asr_tool = 'whisper-large-v3'\n",
    "\n",
    "    @staticmethod\n",
    "    def transcribe(audio_path: Path | str) -> str:\n",
    "        return faster_whisper_helpers.WhisperTranscriber(model=WHISPER_MODEL,\n",
    "                                                         media_filepath=audio_path,\n",
    "                                                         source_language=WHISPER_LANGUAGE_ASR,\n",
    "                                                         condition_on_previous_text=False,\n",
    "                                                         initial_prompt=None,\n",
    "                                                         task='transcribe').transcribe().to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b9d88-c663-45e5-9d16-bb2e4dcf84e3",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8ceded-e379-4427-9a78-72bd75d095b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcription_record(connection: Connection, asr_tool: str, clip_name: str, sentence_id: str,\n",
    "                              transcription: str, duration: float | None) -> None:\n",
    "    transcription_record = {'updated': datetime.datetime.now().astimezone(datetime.timezone.utc),\n",
    "                            'path': clip_name, 'sentence_id': sentence_id, 'transcription': transcription,\n",
    "                            'asr_tool': asr_tool, 'cv_version': COMMON_VOICES_VERSION,\n",
    "                            'duration': duration}\n",
    "    df_transcription = pd.DataFrame([transcription_record]).set_index(['path', 'asr_tool'])\n",
    "    upsert(df=df_transcription, if_row_exists='update', con=connection, table_name='transcriptions',\n",
    "           chunksize=1000, create_table=True)\n",
    "\n",
    "\n",
    "def clip_already_transcribed(connection: Connection, asr_tool: str, clip_name: str):\n",
    "    statement = text(f'''\n",
    "        SELECT EXISTS (\n",
    "            SELECT 1 FROM {TABLE_NAME}\n",
    "            WHERE path = :clip_name\n",
    "            AND asr_tool = :asr_tool\n",
    "            AND transcription IS NOT NULL -- failed transcriptions are set to NULL\n",
    "        )\n",
    "    ''')\n",
    "    parameters = {'clip_name': clip_name, 'asr_tool': asr_tool}\n",
    "\n",
    "    # assume table exists (optimistic approach for saving time) and if we get an error,\n",
    "    # check if it is due to the table not being there\n",
    "    try:\n",
    "        return bool(connection.execute(statement=statement, parameters=parameters).scalar())\n",
    "    except Exception as e:\n",
    "        table_exists = 'transcriptions' in sqla.inspect(connection).get_table_names()\n",
    "        if table_exists:\n",
    "            raise e\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def transcribe_and_save(audio_path: Path | str, connection: Connection, transcriber: Transcriber,\n",
    "                        clip_name: str, sentence_id: str) -> None:\n",
    "    if clip_already_transcribed(connection=connection, asr_tool=transcriber.asr_tool, clip_name=clip_name):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        transcription = transcriber.transcribe(audio_path=audio_path)\n",
    "        end = time.perf_counter()\n",
    "        duration = end - start\n",
    "    except Exception:\n",
    "        logger.exception(f'Failed to transcribe {clip_name}, marking transcription as NULL')\n",
    "        transcription = None\n",
    "        duration = None\n",
    "\n",
    "    save_transcription_record(connection=connection, asr_tool=transcriber.asr_tool,\n",
    "                              clip_name=clip_name, duration=duration,\n",
    "                              sentence_id=sentence_id, transcription=transcription)\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228bee6-bc44-47d5-9c1b-d616a85be1c9",
   "metadata": {},
   "source": [
    "# Get table containing sample clip paths\n",
    "\n",
    "This must have been generated in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67faa431-301c-43f5-9abf-8b4ee453dbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>common_voice_it_32671878.mp3</th>\n",
       "      <td>34d62b82cce0334fc156e6754e9320e8b4d6d09152eb5c...</td>\n",
       "      <td>Beth accetta.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_voice_it_19983792.mp3</th>\n",
       "      <td>0234ccc4d8569b1d653055cb5884924d53b6b13692175f...</td>\n",
       "      <td>Il pezzo ha avuto moltissimo successo nel mondo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_voice_it_23989089.mp3</th>\n",
       "      <td>190f431e0c7145d70702b4e7b8e582dbfb8adc5adb5edc...</td>\n",
       "      <td>La camera funebre ha volta piatta con i lati r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_voice_it_21262721.mp3</th>\n",
       "      <td>1075be9d8faa8faed772694349f4f432b4593f5f5c70da...</td>\n",
       "      <td>Dopo questo album la band si sciolse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>common_voice_it_20003259.mp3</th>\n",
       "      <td>04468068bfee66d8f8075cc871fc2ed5653485a33d56da...</td>\n",
       "      <td>Egli introdusse un nuovo sistema di scommesse ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    sentence_id  \\\n",
       "path                                                                              \n",
       "common_voice_it_32671878.mp3  34d62b82cce0334fc156e6754e9320e8b4d6d09152eb5c...   \n",
       "common_voice_it_19983792.mp3  0234ccc4d8569b1d653055cb5884924d53b6b13692175f...   \n",
       "common_voice_it_23989089.mp3  190f431e0c7145d70702b4e7b8e582dbfb8adc5adb5edc...   \n",
       "common_voice_it_21262721.mp3  1075be9d8faa8faed772694349f4f432b4593f5f5c70da...   \n",
       "common_voice_it_20003259.mp3  04468068bfee66d8f8075cc871fc2ed5653485a33d56da...   \n",
       "\n",
       "                                                                       sentence  \n",
       "path                                                                             \n",
       "common_voice_it_32671878.mp3                                      Beth accetta.  \n",
       "common_voice_it_19983792.mp3   Il pezzo ha avuto moltissimo successo nel mondo.  \n",
       "common_voice_it_23989089.mp3  La camera funebre ha volta piatta con i lati r...  \n",
       "common_voice_it_21262721.mp3              Dopo questo album la band si sciolse.  \n",
       "common_voice_it_20003259.mp3  Egli introdusse un nuovo sistema di scommesse ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_clip_paths = text(f\"SELECT * FROM samples_{LANGUAGE}\")\n",
    "df_commons_sample = pd.read_sql(sql=query_clip_paths, con=ENGINE, index_col='path')\n",
    "df_commons_sample['full_path'] = df_commons_sample.index.map(lambda p: (BASE_PATH / \"clips\") / p)\n",
    "df_commons_sample.drop(columns=['full_path']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bd722-4033-4a8b-89de-ac28aa5352a8",
   "metadata": {},
   "source": [
    "# Transcribe sample clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1422fb-13c7-4cc3-bf06-e2c4f2057cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = len(df_commons_sample)\n",
    "transcribers = (AutosubTranscriber(), WhisperTranscriber())\n",
    "\n",
    "\n",
    "# iterate over all selected audio clips from the common voices dataset\n",
    "with ENGINE.connect() as connection:\n",
    "    for ix, row in enumerate(df_commons_sample.itertuples()):\n",
    "\n",
    "        # transcribe and save result for each tool\n",
    "        for transcriber in transcribers:\n",
    "            transcribe_and_save(audio_path=row.full_path, connection=connection, transcriber=transcriber,\n",
    "                                clip_name=row.Index, sentence_id=row.sentence_id)\n",
    "\n",
    "        # show progress inline\n",
    "        print(f'{ix + 1}/{nb_samples} done', end='\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "asr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
